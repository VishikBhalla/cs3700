## High Level Approach
We concluded that our webcrawler would need to preform two main tasks: the first was logging into Fakebook, and the second was crawling the Fakebook domain. To login, we first sent a GET request to '/accounts/login/?next=/fakebook/' under host 'fring.ccs.neu.edu' and then listened for a response. In our implementation of receiving a message, we parsed the response for HTTP headers, and depending on which ones we received, we handled the message differently. If a Content-Length header was included, we read that much data from the socket. If a Transfer-Encoding header was included, we processed each chunk we received, otherwise we read from the socket until there was nothing left to receive. Once the response was parsed, we extracted the cookies necessary to login, and used them along with the username and password command line arguments to construct a HTTP POST request to submit the form data. Upon receiving a message with Status-Code 200, we completed the login process successfully. Next we crawled through Fakebook. We implemented a HTMLParser that parsed through response data and compiled a queue of links extracted from the HTML. The parser is also responsible for extracting the data from DOM object with the class 'secret_flag' and printing them out. For every link popped off of the queue, a request for the webpage is built and sent, and the response is parsed. If the status of the response is 200, the address is marked as visited and the data is parsed for more links and secret flags. If a 500 status is received, the link is added back to the queue and we continue. If a 301 is received, the new address is obtained from the Location header in that response. If a 400 class error is received, the link is marked as visited. If a connection closed header is encountered, the socket is reopened. Once all 5 secret flags have been found, the program terminates.

## Challenges Faced
Figuring out sending HTTP requests was the first challenge we faced. Through trial and error, we tested sending GET requests to the server and receiving a response. The same was done for the POST request. We encountered connectivity problems that we resolved by adding the necessary headers, such as the Connection: keep-alive header. Once we mastered sending requests and receiving responses, adding the necessary foundation to parse the html took time, but was generally straightforward.

##Testing
We tested the webcrawler incrementally, first testing sending GET requests and receiving the correct response. Print statements were used to debug our client side logic and variables. Once we tested sending HTTP GETs, we tested sending POST requests. The messages received from the server contained status codes and  other headers that helped us to test our request construction until we sent the correct request and were successfully logged in. At this point we had everything we needed to request the pages for every link we encountered, and we print the secret flags when we encounter them, verifying that 5 are printed before the program exited.
