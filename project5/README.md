## High Level Approach
The code is based around an event loop, which reads messages and delegates action for each message type to a corresponding function. Global variables track leader/candidate/follower state as well as various other important state variables. Since the first part of the protocol is the election process, we tackled that first. We followed the steps laid out in the paper (In Search of an Understandable Consensus Algorithm (Extended Version), Diego Ongaro and John Ousterhout) and each replica transitions from follower to candidate until ultimately one receives a majority of votes during a single term and is elected to the leader position. Once we elected a leader, we were able to start servicing client requests. When the leader receives a command to be executed, it adds it to its log and then forwards AppendEntries RPCs to the other servers. Upon achieving quorum, the leader applies the command to the state machine and lets the client know. At this point it is committed. The leader keeps track of the index of this entry and forwards it in heartbeats to the other servers, so that they can apply that log entry to their state machines, and previous entries as well. We then worked on the safety and restrictions for the protocol, including reelections, election restrictions, and follower and candidate crashes. 

## Challenges 
Getting the election protocol in place was the first challenge faced. One bug that took a while to track down was a spontaneous reelection that was entered. The source of the bug was that the leader itself was initiating the reelection because it was timing out; because it was the leader it never received messages from the leader. We disabled the check on the leader and things proceeded smoothly. Additionally, though Raft is undoubtedly easier to understand than Paxos, it still presented a challenge to understanding. Another challenge was figuring out what priority a request had based on its type. For instance, we determined that it was important to always respond to the client and replicas first before continuing on in the main loop, effectively creating a queue from multiple buffers that we serviced first before continuing reading from the socket. After that, based on the server's role, we delegated to other functions that contained the bulk of the safety and restriction checking. Another challenge we faced was when running some of the more advanced tests, after the hard partition ended and the other servers were able to be reached again, the client received a few incorrect get responses. Since the leader couldn't reach quorum on the side of the partition it was on, it would timeout, and the logic in place that would make sure the commands it couldn't obtain quorum for were not committed had a bug leading to the clients receiving incorrect information about what was actually stored on the state machine. This was near the very end of the project, and the protocol was working nearly perfectly at this point except for edge cases like this. 

## Testing
We tested along the way with print statements. A call to our debug function was used wherever we wanted to print out additional information. The function added the replica id and a timestamp to log messages so that they could be distinguished from each other. Each replica also printed in its own color to further distinguish them and make reading the log easier (it's also really pretty, give it a try!). The testing scripts were also run against our code to help us further determine under what circumstances our code would fail. They helped us discover edge cases in our protocol after we achieved basic functionality. We also used those to examine our latency and total messages sent and understand the effect that changing constants like our timeouts had on them.
